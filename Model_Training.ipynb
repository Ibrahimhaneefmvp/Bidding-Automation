{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-02-09T21:01:42.081027Z",
     "iopub.status.busy": "2025-02-09T21:01:42.080742Z",
     "iopub.status.idle": "2025-02-09T21:01:42.091770Z",
     "shell.execute_reply": "2025-02-09T21:01:42.090923Z",
     "shell.execute_reply.started": "2025-02-09T21:01:42.080996Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/adobe-devcraft/dataset/bid.12.txt\n",
      "/kaggle/input/adobe-devcraft/dataset/conv.10.txt\n",
      "/kaggle/input/adobe-devcraft/dataset/bid.09.txt\n",
      "/kaggle/input/adobe-devcraft/dataset/conv.07.txt\n",
      "/kaggle/input/adobe-devcraft/dataset/imp.08.txt\n",
      "/kaggle/input/adobe-devcraft/dataset/conv.06.txt\n",
      "/kaggle/input/adobe-devcraft/dataset/conv.08.txt\n",
      "/kaggle/input/adobe-devcraft/dataset/imp.10.txt\n",
      "/kaggle/input/adobe-devcraft/dataset/clk.09.txt\n",
      "/kaggle/input/adobe-devcraft/dataset/bid.10.txt\n",
      "/kaggle/input/adobe-devcraft/dataset/imp.09.txt\n",
      "/kaggle/input/adobe-devcraft/dataset/imp.07.txt\n",
      "/kaggle/input/adobe-devcraft/dataset/conv.12.txt\n",
      "/kaggle/input/adobe-devcraft/dataset/bid.08.txt\n",
      "/kaggle/input/adobe-devcraft/dataset/conv.09.txt\n",
      "/kaggle/input/adobe-devcraft/dataset/clk.07.txt\n",
      "/kaggle/input/adobe-devcraft/dataset/imp.12.txt\n",
      "/kaggle/input/adobe-devcraft/dataset/imp.06.txt\n",
      "/kaggle/input/adobe-devcraft/dataset/imp.11.txt\n",
      "/kaggle/input/adobe-devcraft/dataset/bid.06.txt\n",
      "/kaggle/input/adobe-devcraft/dataset/clk.11.txt\n",
      "/kaggle/input/adobe-devcraft/dataset/clk.10.txt\n",
      "/kaggle/input/adobe-devcraft/dataset/bid.11.txt\n",
      "/kaggle/input/adobe-devcraft/dataset/clk.06.txt\n",
      "/kaggle/input/adobe-devcraft/dataset/clk.08.txt\n",
      "/kaggle/input/adobe-devcraft/dataset/clk.12.txt\n",
      "/kaggle/input/adobe-devcraft/dataset/bid.07.txt\n",
      "/kaggle/input/adobe-devcraft/dataset/conv.11.txt\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-09T21:01:44.644952Z",
     "iopub.status.busy": "2025-02-09T21:01:44.644640Z",
     "iopub.status.idle": "2025-02-09T21:12:52.589044Z",
     "shell.execute_reply": "2025-02-09T21:12:52.588283Z",
     "shell.execute_reply.started": "2025-02-09T21:01:44.644927Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading and processing data...\n",
      "Loading data files in chunks...\n",
      "Processing impression data...\n",
      "Processing click data...\n",
      "Processing conversion data...\n",
      "Processing bid data...\n",
      "\n",
      "Data loading completed:\n",
      "Total bids: 9,586,949\n",
      "Impressions: 1,815,075 (18.93%)\n",
      "Clicks: 1,159 (0.0121%)\n",
      "Conversions: 38 (0.0004%)\n",
      "\n",
      "Extracting features in chunks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 192/192 [07:10<00:00,  2.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final feature count: 23\n",
      "\n",
      "Splitting data...\n",
      "Train set: 7,669,559 rows\n",
      "Validation set: 958,695 rows\n",
      "Test set: 958,695 rows\n",
      "\n",
      "Training CTR model...\n",
      "[LightGBM] [Info] Number of positive: 912, number of negative: 7668647\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.599197 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1229\n",
      "[LightGBM] [Info] Number of data points in the train set: 7669559, number of used features: 22\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[37]\tvalid_0's auc: 0.88555\tvalid_0's binary_logloss: 0.419999\n",
      "\n",
      "Training CVR model...\n",
      "[LightGBM] [Info] Number of positive: 28, number of negative: 884\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000211 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1132\n",
      "[LightGBM] [Info] Number of data points in the train set: 912, number of used features: 19\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Early stopping, best iteration is:\n",
      "[17]\tvalid_0's auc: 0.936544\tvalid_0's binary_logloss: 0.371248\n",
      "\n",
      "Training market price model...\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.078190 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1249\n",
      "[LightGBM] [Info] Number of data points in the train set: 1452368, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 74.693347\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[100]\tvalid_0's rmse: 47.9317\tvalid_0's l2: 2297.45\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's rmse: 47.9317\tvalid_0's l2: 2297.45\n",
      "\n",
      "Evaluating models...\n",
      "\n",
      "Test Set Results:\n",
      "CTR Model - AUC: 0.8803, PR-AUC: 0.0049\n",
      "CVR Model - AUC: 0.8516, PR-AUC: 0.5911\n",
      "Market Price Model - RMSE: 47.9205\n",
      "\n",
      "Top 10 Important Features for CTR prediction:\n",
      "             feature  importance\n",
      "5           AdslotID         169\n",
      "14              hour         127\n",
      "10  Adslotfloorprice         121\n",
      "3             Domain         114\n",
      "4                URL          86\n",
      "6        Adslotwidth          68\n",
      "1               City          67\n",
      "13      AdvertiserID          60\n",
      "17         is_mobile          60\n",
      "11        CreativeID          53\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, mean_squared_error\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "\n",
    "class RTBDataProcessor:\n",
    "    def __init__(self):\n",
    "        self.label_encoders = {}\n",
    "        self.scaler = StandardScaler()\n",
    "        self.column_names = [\n",
    "            \"BidID\", \"Timestamp\", \"Logtype\", \"VisitorID\", \"User-Agent\", \"IP\", \"Region\", \"City\",\n",
    "            \"Adexchange\", \"Domain\", \"URL\", \"AnonymousURLID\", \"AdslotID\", \"Adslotwidth\", \"Adslotheight\",\n",
    "            \"Adslotvisibility\", \"Adslotformat\", \"Adslotfloorprice\", \"CreativeID\", \"Biddingprice\",\n",
    "            \"Payingprice\", \"KeypageURL\", \"AdvertiserID\"\n",
    "        ]\n",
    "        self.column_names_bid = [\n",
    "            \"BidID\", \"Timestamp\", \"VisitorID\", \"User-Agent\", \"IP\", \"Region\", \"City\",\n",
    "            \"Adexchange\", \"Domain\", \"URL\", \"AnonymousURLID\", \"AdslotID\", \"Adslotwidth\", \"Adslotheight\",\n",
    "            \"Adslotvisibility\", \"Adslotformat\", \"Adslotfloorprice\", \"CreativeID\", \"Biddingprice\",\n",
    "            \"AdvertiserID\"\n",
    "        ]\n",
    "\n",
    "    def load_data(self, bid_path, imp_path, clk_path, conv_path, chunksize=100000):\n",
    "        print(\"Loading data files in chunks...\")\n",
    "        def read_data_chunks(file_path, columns):\n",
    "            chunks = []\n",
    "            for chunk in pd.read_csv(file_path, sep='\\t', header=None, chunksize=chunksize):\n",
    "                chunk = chunk.iloc[:, :-1]  \n",
    "                chunk.columns = columns\n",
    "                chunks.append(chunk['BidID'])\n",
    "            return pd.concat(chunks)\n",
    "    \n",
    "        bid_ids = read_data_chunks(bid_path, self.column_names_bid)\n",
    "        imp_ids = read_data_chunks(imp_path, self.column_names)\n",
    "        clk_ids = read_data_chunks(clk_path, self.column_names)\n",
    "        conv_ids = read_data_chunks(conv_path, self.column_names)\n",
    "    \n",
    "        labels_df = pd.DataFrame({'BidID': pd.concat([bid_ids, imp_ids, clk_ids, conv_ids]).unique()})\n",
    "        \n",
    " \n",
    "        del bid_ids, imp_ids, clk_ids, conv_ids\n",
    "        gc.collect()\n",
    "    \n",
    "        labels_df['is_impression'] = 0\n",
    "        labels_df['is_click'] = 0\n",
    "        labels_df['is_conversion'] = 0\n",
    "        labels_df['Payingprice'] = 0.0\n",
    "    \n",
    "        \n",
    "        print(\"Processing impression data...\")\n",
    "        for chunk in pd.read_csv(imp_path, sep='\\t', header=None, chunksize=chunksize):\n",
    "            chunk = chunk.iloc[:, :-1]\n",
    "            chunk.columns = self.column_names\n",
    "            chunk = chunk.drop_duplicates(subset='BidID', keep='first')\n",
    "            temp_df = chunk[['BidID', 'Payingprice']].set_index('BidID')\n",
    "            mask = labels_df['BidID'].isin(chunk['BidID'])\n",
    "            labels_df.loc[mask, 'is_impression'] = 1\n",
    "            matching_prices = temp_df.loc[labels_df.loc[mask, 'BidID']]['Payingprice'].values\n",
    "            labels_df.loc[mask, 'Payingprice'] = matching_prices\n",
    "            del chunk, temp_df\n",
    "            gc.collect()\n",
    "\n",
    "        print(\"Processing click data...\")\n",
    "        for chunk in pd.read_csv(clk_path, sep='\\t', header=None, chunksize=chunksize):\n",
    "            chunk = chunk.iloc[:, :-1]\n",
    "            chunk.columns = self.column_names\n",
    "            chunk = chunk.drop_duplicates(subset='BidID', keep='first')\n",
    "            mask = labels_df['BidID'].isin(chunk['BidID'])\n",
    "            labels_df.loc[mask, 'is_click'] = 1\n",
    "            del chunk\n",
    "            gc.collect()\n",
    "    \n",
    "        print(\"Processing conversion data...\")\n",
    "        for chunk in pd.read_csv(conv_path, sep='\\t', header=None, chunksize=chunksize):\n",
    "            chunk = chunk.iloc[:, :-1]\n",
    "            chunk.columns = self.column_names\n",
    "            chunk = chunk.drop_duplicates(subset='BidID', keep='first')\n",
    "            mask = labels_df['BidID'].isin(chunk['BidID'])\n",
    "            labels_df.loc[mask, 'is_conversion'] = 1\n",
    "            del chunk\n",
    "            gc.collect()\n",
    "    \n",
    "        print(\"Processing bid data...\")\n",
    "        bid_chunks = []\n",
    "        for chunk in pd.read_csv(bid_path, sep='\\t', header=None, chunksize=chunksize):\n",
    "            chunk = chunk.iloc[:, :-1]\n",
    "            chunk.columns = self.column_names_bid\n",
    "            chunk = chunk.drop_duplicates(subset='BidID', keep='first')\n",
    "            bid_chunks.append(chunk)\n",
    "            \n",
    "        bid_df = pd.concat(bid_chunks)\n",
    "        del bid_chunks\n",
    "        gc.collect()\n",
    "    \n",
    "        print(\"\\nData loading completed:\")\n",
    "        print(f\"Total bids: {len(labels_df):,}\")\n",
    "        print(f\"Impressions: {labels_df['is_impression'].sum():,} ({labels_df['is_impression'].mean()*100:.2f}%)\")\n",
    "        print(f\"Clicks: {labels_df['is_click'].sum():,} ({labels_df['is_click'].mean()*100:.4f}%)\")\n",
    "        print(f\"Conversions: {labels_df['is_conversion'].sum():,} ({labels_df['is_conversion'].mean()*100:.4f}%)\")\n",
    "    \n",
    "        return bid_df, labels_df\n",
    "\n",
    "    def extract_features(self, df, chunk_size=50000):\n",
    "        print(\"\\nExtracting features in chunks...\")\n",
    "        \n",
    "        feature_chunks = []\n",
    "        for i in tqdm(range(0, len(df), chunk_size)):\n",
    "            chunk = df.iloc[i:i + chunk_size].copy()\n",
    "            \n",
    "            \n",
    "            chunk['Timestamp'] = pd.to_datetime(chunk['Timestamp'], format='%Y%m%d%H%M%S%f', errors='coerce')\n",
    "            chunk['hour'] = chunk['Timestamp'].dt.hour.fillna(-1).astype(np.int8)\n",
    "            chunk['day_of_week'] = chunk['Timestamp'].dt.dayofweek.fillna(-1).astype(np.int8)\n",
    "            chunk['is_weekend'] = chunk['day_of_week'].isin([5, 6]).astype(np.int8)\n",
    "            \n",
    "         \n",
    "            chunk['User-Agent'] = chunk['User-Agent'].fillna('unknown')\n",
    "            chunk['is_mobile'] = chunk['User-Agent'].str.contains('Mobile|Android|iOS', case=False, na=False).astype(np.int8)\n",
    "            chunk['is_chrome'] = chunk['User-Agent'].str.contains('Chrome', case=False, na=False).astype(np.int8)\n",
    "            chunk['is_firefox'] = chunk['User-Agent'].str.contains('Firefox', case=False, na=False).astype(np.int8)\n",
    "            chunk['is_safari'] = chunk['User-Agent'].str.contains('Safari', case=False, na=False).astype(np.int8)\n",
    "            \n",
    "         \n",
    "            chunk['Adslotwidth'] = pd.to_numeric(chunk['Adslotwidth'], errors='coerce').fillna(0).astype(np.float32)\n",
    "            chunk['Adslotheight'] = pd.to_numeric(chunk['Adslotheight'], errors='coerce').fillna(0).astype(np.float32)\n",
    "            chunk['ad_area'] = (chunk['Adslotwidth'] * chunk['Adslotheight']).astype(np.float32)\n",
    "            chunk['is_premium_ad'] = (chunk['ad_area'] >= 100000).astype(np.int8)\n",
    "            \n",
    "          \n",
    "            cat_cols = ['Region', 'City', 'Adexchange', 'Domain', 'URL', 'AdslotID',\n",
    "                       'Adslotvisibility', 'Adslotformat', 'CreativeID', 'AdvertiserID']\n",
    "            \n",
    "            for col in cat_cols:\n",
    "                if col in chunk.columns:\n",
    "                    chunk[col] = chunk[col].fillna('unknown')\n",
    "                    if col not in self.label_encoders:\n",
    "                        self.label_encoders[col] = LabelEncoder()\n",
    "                        self.label_encoders[col].fit(df[col].fillna('unknown').astype(str))\n",
    "                    chunk[col] = self.label_encoders[col].transform(chunk[col].astype(str))\n",
    "                    chunk[col] = chunk[col].astype(np.int32)\n",
    "            \n",
    "           \n",
    "            drop_cols = ['Timestamp', 'User-Agent', 'IP', 'AnonymousURLID', 'KeypageURL', 'BidID','VisitorID']\n",
    "            chunk = chunk.drop(columns=[col for col in drop_cols if col in chunk.columns])\n",
    "            \n",
    "            feature_chunks.append(chunk)\n",
    "            \n",
    "    \n",
    "            del chunk\n",
    "            gc.collect()    \n",
    "\n",
    "        features = pd.concat(feature_chunks, axis=0)\n",
    "        del feature_chunks\n",
    "        gc.collect()\n",
    "        \n",
    "        num_cols = ['Adslotwidth', 'Adslotheight', 'ad_area', 'Adslotfloorprice']\n",
    "        features[num_cols] = self.scaler.fit_transform(features[num_cols])\n",
    "        \n",
    "        print(f\"Final feature count: {len(features.columns)}\")\n",
    "        return features\n",
    "\n",
    "def optimize_dtypes(df):\n",
    "    for col in df.columns:\n",
    "        # Optimize integers\n",
    "        if df[col].dtype == 'int64':\n",
    "            if df[col].min() >= 0:\n",
    "                if df[col].max() < 255:\n",
    "                    df[col] = df[col].astype(np.uint8)\n",
    "                elif df[col].max() < 65535:\n",
    "                    df[col] = df[col].astype(np.uint16)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.uint32)\n",
    "            else:\n",
    "                if df[col].min() > -128 and df[col].max() < 127:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif df[col].min() > -32768 and df[col].max() < 32767:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "        \n",
    "        # Optimize floats\n",
    "        elif df[col].dtype == 'float64':\n",
    "            df[col] = df[col].astype(np.float32)\n",
    "            \n",
    "    return df\n",
    "def train_and_evaluate():\n",
    "    # Initialize processor\n",
    "    processor = RTBDataProcessor()\n",
    "    \n",
    "    # Load data\n",
    "    print(\"\\nLoading and processing data...\")\n",
    "    bid_df, labels_df = processor.load_data(\n",
    "        '/kaggle/input/adobe-devcraft/dataset/bid.06.txt',\n",
    "        '/kaggle/input/adobe-devcraft/dataset/imp.06.txt',\n",
    "        '/kaggle/input/adobe-devcraft/dataset/clk.06.txt',\n",
    "        '/kaggle/input/adobe-devcraft/dataset/conv.06.txt'\n",
    "    )\n",
    "    \n",
    "    # Process features\n",
    "    features_df = processor.extract_features(bid_df,chunk_size = 50000)\n",
    "    features_df = optimize_dtypes(features_df)\n",
    "    labels_df = optimize_dtypes(labels_df)\n",
    "    full_df = pd.concat([features_df, labels_df.drop('BidID', axis=1)], axis=1)\n",
    "    \n",
    "    # Split data\n",
    "    print(\"\\nSplitting data...\")\n",
    "    train_df, test_df = train_test_split(full_df, test_size=0.2, random_state=42)\n",
    "    val_df, test_df = train_test_split(test_df, test_size=0.5, random_state=42)\n",
    "    \n",
    "    print(f\"Train set: {len(train_df):,} rows\")\n",
    "    print(f\"Validation set: {len(val_df):,} rows\")\n",
    "    print(f\"Test set: {len(test_df):,} rows\")\n",
    "    \n",
    "    # Define feature columns\n",
    "    feature_cols = features_df.columns.tolist()\n",
    "    \n",
    "    # Train CTR model\n",
    "    print(\"\\nTraining CTR model...\")\n",
    "    ctr_model = lgb.LGBMClassifier(\n",
    "        n_estimators=100,\n",
    "        learning_rate=0.05,\n",
    "        max_depth=7,\n",
    "        num_leaves=31,\n",
    "        min_child_samples=20,\n",
    "        class_weight='balanced',\n",
    "        random_state=42\n",
    "    )\n",
    "    eval_set = [(val_df[feature_cols], val_df['is_click'])]\n",
    "    \n",
    "    ctr_model.fit(\n",
    "        train_df[feature_cols],\n",
    "        train_df['is_click'],\n",
    "        eval_set=eval_set,\n",
    "        eval_metric='auc',\n",
    "        callbacks=[\n",
    "            lgb.early_stopping(stopping_rounds=10),\n",
    "            lgb.log_evaluation(period=100)\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Train CVR model\n",
    "    print(\"\\nTraining CVR model...\")\n",
    "    cvr_model = lgb.LGBMClassifier(\n",
    "        n_estimators=100,\n",
    "        learning_rate=0.05,\n",
    "        max_depth=7,\n",
    "        num_leaves=31,\n",
    "        min_child_samples=20,\n",
    "        class_weight='balanced',\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Train only on clicked impressions\n",
    "    clicked_train = train_df[train_df['is_click'] == 1]\n",
    "    clicked_val = val_df[val_df['is_click'] == 1]\n",
    "    \n",
    "    if len(clicked_train) > 0:\n",
    "        eval_set = [(clicked_val[feature_cols], clicked_val['is_conversion'])]\n",
    "        cvr_model.fit(\n",
    "            clicked_train[feature_cols],\n",
    "            clicked_train['is_conversion'],\n",
    "            eval_set=eval_set,\n",
    "            eval_metric='auc',\n",
    "            callbacks=[\n",
    "                lgb.early_stopping(stopping_rounds=10),\n",
    "                lgb.log_evaluation(period=100)\n",
    "            ]\n",
    "        )\n",
    "    \n",
    "    # Train market price model\n",
    "    print(\"\\nTraining market price model...\")\n",
    "    market_price_model = lgb.LGBMRegressor(\n",
    "        n_estimators=100,\n",
    "        learning_rate=0.05,\n",
    "        max_depth=7,\n",
    "        num_leaves=31,\n",
    "        min_child_samples=20,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Train only on won impressions\n",
    "    won_train = train_df[train_df['is_impression'] == 1]\n",
    "    won_val = val_df[val_df['is_impression'] == 1]\n",
    "    eval_set = [(won_val[feature_cols], won_val['Payingprice'])]\n",
    "    market_price_model.fit(\n",
    "        won_train[feature_cols],\n",
    "        won_train['Payingprice'],\n",
    "        eval_set=eval_set,\n",
    "        eval_metric='rmse',\n",
    "        callbacks=[\n",
    "            lgb.early_stopping(stopping_rounds=10),\n",
    "            lgb.log_evaluation(period=100)\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Evaluate models\n",
    "    print(\"\\nEvaluating models...\")\n",
    "    \n",
    "    # CTR predictions\n",
    "    ctr_pred = ctr_model.predict_proba(test_df[feature_cols])[:, 1]\n",
    "    ctr_auc = roc_auc_score(test_df['is_click'], ctr_pred)\n",
    "    ctr_prauc = average_precision_score(test_df['is_click'], ctr_pred)\n",
    "    \n",
    "    # CVR predictions (only for clicked impressions)\n",
    "    clicked_test = test_df[test_df['is_click'] == 1]\n",
    "    if len(clicked_test) > 0:\n",
    "        cvr_pred = cvr_model.predict_proba(clicked_test[feature_cols])[:, 1]\n",
    "        cvr_auc = roc_auc_score(clicked_test['is_conversion'], cvr_pred)\n",
    "        cvr_prauc = average_precision_score(clicked_test['is_conversion'], cvr_pred)\n",
    "    \n",
    "    # Market price predictions (only for won impressions)\n",
    "    won_test = test_df[test_df['is_impression'] == 1]\n",
    "    market_pred = market_price_model.predict(won_test[feature_cols])\n",
    "    market_rmse = np.sqrt(mean_squared_error(won_test['Payingprice'], market_pred))\n",
    "    \n",
    "    # Print results\n",
    "    print(\"\\nTest Set Results:\")\n",
    "    print(f\"CTR Model - AUC: {ctr_auc:.4f}, PR-AUC: {ctr_prauc:.4f}\")\n",
    "    if len(clicked_test) > 0:\n",
    "        print(f\"CVR Model - AUC: {cvr_auc:.4f}, PR-AUC: {cvr_prauc:.4f}\")\n",
    "    print(f\"Market Price Model - RMSE: {market_rmse:.4f}\")\n",
    "    \n",
    "    # Feature importance analysis\n",
    "    print(\"\\nTop 10 Important Features for CTR prediction:\")\n",
    "    feature_imp = pd.DataFrame({\n",
    "        'feature': feature_cols,\n",
    "        'importance': ctr_model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    print(feature_imp.head(10))\n",
    "    \n",
    "    return ctr_model, cvr_model, market_price_model, processor\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    models = train_and_evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-09T21:14:43.380434Z",
     "iopub.status.busy": "2025-02-09T21:14:43.380112Z",
     "iopub.status.idle": "2025-02-09T21:14:43.384334Z",
     "shell.execute_reply": "2025-02-09T21:14:43.383430Z",
     "shell.execute_reply.started": "2025-02-09T21:14:43.380404Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "ctr_model, cvr_model, market_price_model, processor = models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-09T21:15:55.076828Z",
     "iopub.status.busy": "2025-02-09T21:15:55.076534Z",
     "iopub.status.idle": "2025-02-09T21:15:55.531658Z",
     "shell.execute_reply": "2025-02-09T21:15:55.530931Z",
     "shell.execute_reply.started": "2025-02-09T21:15:55.076806Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models and preprocessing objects saved.\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "joblib.dump(ctr_model, 'ctr_model.pkl')\n",
    "joblib.dump(cvr_model, 'cvr_model.pkl')\n",
    "joblib.dump(market_price_model, 'market_price_model.pkl')\n",
    "joblib.dump(processor.label_encoders, 'label_encoders.pkl')\n",
    "joblib.dump(processor.scaler, 'scaler.pkl')\n",
    "\n",
    "print(\"Models and preprocessing objects saved.\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6610473,
     "sourceId": 10672493,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30839,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
